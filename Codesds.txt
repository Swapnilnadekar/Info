Practical 1-----
#Predict the price of the Uber ride from a given pickup point to the agreed drop-off location. Perform following tasks:
Pre-process the dataset.
Identify outliers.
Check the correlation.
Implement linear regression and random forest regression models.
Evaluate the models and compare their respective scores like R2, RMSE, etc. Dataset link: https://www.kaggle.com/datasets/yasserh/uber-fares-dataset

#Importing the required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



#importing the dataset
df  = pd.read_csv("uber.csv")


df.head()


df.info()             #To get the required information of the dataset


df.columns            #TO get number of columns in the dataset


df = df.drop(['Unnamed: 0', 'key'], axis= 1)         #To drop unnamed column as it isn't required


df.head()


df.shape #To get the total (Rows,Columns)



df.dtypes #To get the type of each column



df.info()



df.describe() #To get statistics of each columns 



df.isnull().sum() 


df['dropoff_latitude'].fillna(value=df['dropoff_latitude'].mean(),inplace = True)
df['dropoff_longitude'].fillna(value=df['dropoff_longitude'].median(),inplace = True)



df.isnull().sum() 


df.dtypes



df.pickup_datetime = pd.to_datetime(df.pickup_datetime, errors='coerce') 



df.dtypes



df= df.assign(hour = df.pickup_datetime.dt.hour,
             day= df.pickup_datetime.dt.day,
             month = df.pickup_datetime.dt.month,
             year = df.pickup_datetime.dt.year,
             dayofweek = df.pickup_datetime.dt.dayofweek)



df.head()



# drop the column 'pickup_daetime' using drop()
# 'axis = 1' drops the specified column

df = df.drop('pickup_datetime',axis=1)



df.head()



df.dtypes



df.plot(kind = "box",subplots = True,layout = (7,2),figsize=(15,20)) #Boxplot to check the outliers



#Using the InterQuartile Range to fill the values
def remove_outlier(df1 , col):
    Q1 = df1[col].quantile(0.25)
    Q3 = df1[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_whisker = Q1-1.5*IQR
    upper_whisker = Q3+1.5*IQR
    df[col] = np.clip(df1[col] , lower_whisker , upper_whisker)
    return df1

def treat_outliers_all(df1 , col_list):
    for c in col_list:
        df1 = remove_outlier(df , c)
    return df1



df = treat_outliers_all(df , df.iloc[: , 0::])




df.plot(kind = "box",subplots = True,layout = (7,2),figsize=(15,20)) #Boxplot shows that dataset is free from outliers




#pip install haversine
import haversine as hs  #Calculate the distance using Haversine to calculate the distance between to points. Can't use Eucladian as it is for flat surface.
travel_dist = []
for pos in range(len(df['pickup_longitude'])):
        long1,lati1,long2,lati2 = [df['pickup_longitude'][pos],df['pickup_latitude'][pos],df['dropoff_longitude'][pos],df['dropoff_latitude'][pos]]
        loc1=(lati1,long1)
        loc2=(lati2,long2)
        c = hs.haversine(loc1,loc2)
        travel_dist.append(c)
    
print(travel_dist)
df['dist_travel_km'] = travel_dist
df.head()



#Uber doesn't travel over 130 kms so minimize the distance 
df= df.loc[(df.dist_travel_km >= 1) | (df.dist_travel_km <= 130)]
print("Remaining observastions in the dataset:", df.shape)



#Finding inccorect latitude (Less than or greater than 90) and longitude (greater than or less than 180)
incorrect_coordinates = df.loc[(df.pickup_latitude > 90) |(df.pickup_latitude < -90) |
                                   (df.dropoff_latitude > 90) |(df.dropoff_latitude < -90) |
                                   (df.pickup_longitude > 180) |(df.pickup_longitude < -180) |
                                   (df.dropoff_longitude > 90) |(df.dropoff_longitude < -90)
                                    ]




df.drop(incorrect_coordinates, inplace = True, errors = 'ignore')



df.head()



df.isnull().sum()


sns.heatmap(df.isnull()) #Free for null values



corr = df.corr() #Function to find the correlation



corr



fig,axis = plt.subplots(figsize = (10,6))
sns.heatmap(df.corr(),annot = True) #Correlation Heatmap (Light values means highly correlated)



x = df[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','hour','day','month','year','dayofweek','dist_travel_km']]


y = df['fare_amount']



from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.33)



from sklearn.linear_model import LinearRegression
regression = LinearRegression()



regression.fit(X_train,y_train)


regression.intercept_ #To find the linear intercept


regression.coef_ #To find the linear coeeficient


prediction = regression.predict(X_test) #To predict the target values


print(prediction)



y_test



from sklearn.metrics import r2_score 


r2_score(y_test,prediction)



from sklearn.metrics import mean_squared_error


MSE = mean_squared_error(y_test,prediction)


MSE 



RMSE = np.sqrt(MSE)



RMSE



from sklearn.ensemble import RandomForestRegressor



rf = RandomForestRegressor(n_estimators=100) #Here n_estimators means number of trees you want to build before making the prediction



rf.fit(X_train,y_train)




y_pred = rf.predict(X_test)



y_pred




R2_Random = r2_score(y_test,y_pred)



R2_Random




MSE_Random = mean_squared_error(y_test,y_pred)




MSE_Random



RMSE_Random = np.sqrt(MSE_Random)



RMSE_Random


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Practical no. 2
                    sales_data_sample.csv



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import zipfile
import cv2
import plotly.express as px


from sklearn.preprocessing import StandardScaler, normalize
from sklearn.cluster import KMeans

%matplotlib inline



df = pd.read_csv('/content/sales_data_sample.csv', encoding = 'unicode_escape', parse_dates=['ORDERDATE'])
df.head()




df.dtypes



df.info()




df.isna().mean()



df_drop  = ['ADDRESSLINE1', 'ADDRESSLINE2', 'POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']
df = df.drop(df_drop, axis=1)
df.head(3)



df.shape



df.isna().sum()




def barplot_visualization(x):
  fig = plt.Figure(figsize = (12, 6))
  fig = px.bar(x = df[x].value_counts().index, y = df[x].value_counts(), color = df[x].value_counts().index, height = 600)
  fig.show();



barplot_visualization('COUNTRY')




barplot_visualization('STATUS');



df.drop(columns=['STATUS'], axis=1, inplace=True)




print('Columns resume: ', df.shape[1])





barplot_visualization('PRODUCTLINE')


barplot_visualization('DEALSIZE')



def dummies(x):
  dummy = pd.get_dummies(df[x])
  df.drop(columns=x, inplace=True)
  return pd.concat([df, dummy], axis = 1)




df =  dummies('COUNTRY')



df =  dummies('PRODUCTLINE')



df =  dummies('DEALSIZE')



df.head()



y = pd.Categorical(df['PRODUCTCODE'])
y




df['PRODUCTCODE'] = pd.Categorical(df['PRODUCTCODE']).codes



df.head()




df_group = df.groupby(by='ORDERDATE').sum()
fig = px.line(x = df_group.index, y = df_group.SALES, title='sales_peak')
fig.show();




df.drop('ORDERDATE', axis=1, inplace=True)



df.drop('QTR_ID', axis=1, inplace=True)



df.shape




scaler =  StandardScaler()
df_scaled = scaler.fit_transform(df)




scores = []
range_values = range(1, 15)
for i in range_values:
  kmeans = KMeans(n_clusters = i)
  kmeans.fit(df_scaled)
  scores.append(kmeans.inertia_)



plt.plot(scores, 'bx-')
plt.title('Finding right number of clusters')
plt.xlabel('Clusters')
plt.ylabel('scores') 
plt.show();




kmeans = KMeans(4)
kmeans.fit(df_scaled)



labels = kmeans.labels_
labels



kmeans.cluster_centers_.shape




cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns= [df.columns])
cluster_centers



cluster_centers = scaler.inverse_transform(cluster_centers)
cluster_centers = pd.DataFrame(data=cluster_centers, columns=[df.columns])
cluster_centers



sales_of_cluster = pd.concat([df, pd.DataFrame({'cluster': labels})], axis=1)
sales_of_cluster.head()




/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



ML_Assignmet_3_             diabetes.csv



KNN algorithm on diabetes dataset




import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split

from sklearn import metrics



df=pd.read_csv('/content/diabetes.csv')



df.columns



df.isnull().sum()



plt.hist([x, y], color=['red', 'green'], label = ['exit', 'not_exit'])




X = df.drop('Outcome',axis = 1)
y1 = df['Outcome']




x = df[df['Outcome']==1]['BMI']
y= df[df['Outcome']==0]['BMI']



plt.hist([x, y], color=['red', 'green'], label = ['exit', 'not_exit'])




x = df[df['Outcome']==1]['Glucose']
y= df[df['Outcome']==0]['Glucose']




plt.hist([x, y], color=['red', 'green'], label = ['exit', 'not_exit'])




from sklearn.preprocessing import scale
X = scale(X)

# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size = 0.3, random_state = 42)



from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
 
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)



print("Confusion matrix: ")
cs = metrics.confusion_matrix(y_test,y_pred)
print(cs)



print("Acccuracy ",metrics.accuracy_score(y_test,y_pred))



total_misclassified = cs[0,1] + cs[1,0]
print(total_misclassified)
total_examples = cs[0,0]+cs[0,1]+cs[1,0]+cs[1,1]
print(total_examples)
print("Error rate",total_misclassified/total_examples)
print("Error rate ",1-metrics.accuracy_score(y_test,y_pred))




print("Precision score",metrics.precision_score(y_test,y_pred))




print("Recall score ",metrics.recall_score(y_test,y_pred))




print("Classification report ",metrics.classification_report(y_test,y_pred))



/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


ML Practical 4 




Given a bank customer, build a neural network-based classifier that can determine whether they will leave or not in the next 6 months.
Dataset Description: The case study is from an open-source dataset from Kaggle. The dataset contains 10,000 sample points with 14 distinct features such as CustomerId, CreditScore, Geography, Gender, Age, Tenure, Balance, etc. Link to the Kaggle project: https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling Perform following steps:

Read the dataset.
Distinguish the feature and target set and divide the data set into training and test sets.
Normalize the train and test data.
Initialize and build the model. Identify the points of improvement and implement the same.
Print the accuracy score and confusion matrix.


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt #Importing the libraries



df = pd.read_csv("Churn_Modelling.csv")



df.head()



df.shape




df.describe()



df.isnull()




df.isnull().sum()


df.info()


df.dtypes



df.columns



df = df.drop(['RowNumber', 'Surname', 'CustomerId'], axis= 1) #Dropping the unnecessary columns 



df.head()




Visualization




def visualization(x, y, xlabel):
    plt.figure(figsize=(10,5))
    plt.hist([x, y], color=['red', 'green'], label = ['exit', 'not_exit'])
    plt.xlabel(xlabel,fontsize=20)
    plt.ylabel("No. of customers", fontsize=20)
    plt.legend()




df_churn_exited = df[df['Exited']==1]['Tenure']
df_churn_not_exited = df[df['Exited']==0]['Tenure']




visualization(df_churn_exited, df_churn_not_exited, "Tenure")




df_churn_exited2 = df[df['Exited']==1]['Age']
df_churn_not_exited2 = df[df['Exited']==0]['Age']



visualization(df_churn_exited2, df_churn_not_exited2, "Age")




Converting the Categorical Variables




X = df[['CreditScore','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary']]
states = pd.get_dummies(df['Geography'],drop_first = True)
gender = pd.get_dummies(df['Gender'],drop_first = True)





df = pd.concat([df,gender,states], axis = 1)




df.head()





X = df[['CreditScore','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary','Male','Germany','Spain']]



y = df['Exited']




from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.30)





from sklearn.preprocessing import StandardScaler
sc = StandardScaler()





X_train  = sc.fit_transform(X_train)
X_test = sc.transform(X_test)




X_train





X_test




import keras 





from keras.models import Sequential          #To create sequential neural network
from keras.layers import Dense               #To create hidden layers



classifier = Sequential()




classifier.add(Dense(activation = "relu",input_dim = 11,units = 6,kernel_initializer = "uniform")) 



classifier.add(Dense(activation = "relu",units = 6,kernel_initializer = "uniform"))   #Adding second hidden layers




classifier.add(Dense(activation = "sigmoid",units = 1,kernel_initializer = "uniform")) #Final neuron will be having siigmoid function



classifier.compile(optimizer="adam",loss = 'binary_crossentropy',metrics = ['accuracy']) #To compile the Artificial Neural Network. Ussed Binary crossentropy as we just have only two output



classifier.summary() #3 layers created. 6 neurons in 1st,6neurons in 2nd layer and 1 neuron in last




classifier.fit(X_train,y_train,batch_size=10,epochs=50) #Fitting the ANN to training dataset




y_pred =classifier.predict(X_test)
y_pred = (y_pred > 0.5) #Predicting the result



from sklearn.metrics import confusion_matrix,accuracy_score,classification_report



cm = confusion_matrix(y_test,y_pred)




cm




accuracy = accuracy_score(y_test,y_pred)



accuracy




plt.figure(figsize = (10,7))
sns.heatmap(cm,annot = True)
plt.xlabel('Predicted')
plt.ylabel('Truth')





print(classification_report(y_test,y_pred))
              



/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Practical 5
Classify the email using the binary classification method. Email Spam detection has two states: a) Normal State – Not Spam, b) Abnormal State – Spam. Use K-Nearest Neighbors and Support Vector Machine for classification. Analyze their performance. Dataset link: The emails.csv dataset on the Kaggle https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics




df=pd.read_csv('/content/emails.csv',error_bad_lines=False)




df.head()



df.columns



df.isnull().sum()



df.dropna(inplace = True)



df.drop(['Email No.'],axis=1,inplace=True)
X = df.drop(['Prediction'],axis = 1)
y = df['Prediction']




from sklearn.preprocessing import scale
X = scale(X)
# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)



from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
 
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)




print("Prediction",y_pred)



print("KNN accuracy = ",metrics.accuracy_score(y_test,y_pred))




print("Confusion matrix",metrics.confusion_matrix(y_test,y_pred))







# cost C = 1
model = SVC(C = 1)

# fit
model.fit(X_train, y_train)

# predict
y_pred = model.predict(X_test)






metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)




print("SVM accuracy = ",metrics.accuracy_score(y_test,y_pred))